{"cells":[{"cell_type":"markdown","metadata":{},"source":["Installing libraries"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install -qU langgraph langchain langchain_openai langchain_community qdrant-client PyPDF2 beautifulsoup4 requests"]},{"cell_type":"markdown","metadata":{},"source":["Setting up imports"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import os\n","import getpass\n","from typing import List, Dict, Any, Union, Annotated, Optional\n","from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","from langchain.agents import AgentExecutor, create_openai_functions_agent\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.tools import BaseTool, tool\n","from langgraph.graph import StateGraph, END\n","from langchain_community.tools.tavily_search import TavilySearchResults\n","from langchain_community.vectorstores import Qdrant\n","import functools\n","import operator\n","from pathlib import Path\n","import uuid\n","import requests\n","from bs4 import BeautifulSoup\n","import PyPDF2\n","import io\n","from qdrant_client import QdrantClient\n","from qdrant_client.http import models as rest\n","\n","# Set up API keys\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n","os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API Key:\")\n","\n","# Create a working directory\n","WORKING_DIRECTORY = Path(\"./content/data\")\n","WORKING_DIRECTORY.mkdir(parents=True, exist_ok=True)"]},{"cell_type":"markdown","metadata":{},"source":["Initialize Qdrant Client and OpenAI Embeddings"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Initialize Qdrant client\n","qdrant_client = QdrantClient(\":memory:\")  # Use in-memory storage for simplicity\n","\n","# Initialize OpenAI embeddings\n","embeddings = OpenAIEmbeddings()"]},{"cell_type":"markdown","metadata":{},"source":["State and Helper functions"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class State(Dict[str, Any]):\n","    messages: Annotated[List[BaseMessage], operator.add]\n","    next: str\n","    documents: Dict[str, str]\n","\n","def get_last_message(state: State) -> str:\n","    return state[\"messages\"][-1].content\n","\n","def join_graph(response: dict):\n","    return {\"messages\": [response[\"messages\"][-1]]}\n","\n","def prelude(state):\n","    written_files = []\n","    try:\n","        written_files = [f.relative_to(WORKING_DIRECTORY) for f in WORKING_DIRECTORY.glob(\"*\")]\n","    except:\n","        pass\n","    if not written_files:\n","        return {**state, \"current_files\": \"No files written.\"}\n","    return {\n","        **state,\n","        \"current_files\": \"\\nBelow are files your team has written to the directory:\\n\"\n","        + \"\\n\".join([f\" - {f}\" for f in written_files]),\n","    }"]},{"cell_type":"markdown","metadata":{},"source":["Setting up Vector Store"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def initialize_vector_store():\n","    qdrant_client.recreate_collection(\n","        collection_name=\"sales_pitch_data\",\n","        vectors_config=rest.VectorParams(size=1536, distance=rest.Distance.COSINE),\n","    )\n","\n","def add_to_vector_store(text: str, metadata: Dict[str, Any]):\n","    vector = embeddings.embed_query(text)\n","    qdrant_client.upsert(\n","        collection_name=\"sales_pitch_data\",\n","        points=[rest.PointStruct(id=uuid.uuid4().hex, vector=vector, payload=metadata)],\n","    )\n","\n","def query_vector_store(query: str, limit: int = 5) -> List[Dict[str, Any]]:\n","    query_vector = embeddings.embed_query(query)\n","    search_result = qdrant_client.search(\n","        collection_name=\"sales_pitch_data\",\n","        query_vector=query_vector,\n","        limit=limit,\n","    )\n","    return [hit.payload for hit in search_result]"]},{"cell_type":"markdown","metadata":{},"source":["Tools"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["tavily_tool = TavilySearchResults(max_results=5)\n","\n","@tool\n","def linkedin_check(name: str, company: str) -> bool:\n","    \"\"\"Check if a person works at a specific company on LinkedIn.\"\"\"\n","    # Note: This is a mock implementation. In a real scenario, you'd use LinkedIn's API or web scraping.\n","    url = f\"https://www.linkedin.com/search/results/people/?keywords={name} {company}\"\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","    return company.lower() in soup.text.lower()\n","\n","@tool\n","def read_document(\n","    file_name: Annotated[str, \"File path to read the document.\"],\n","    start: Annotated[Optional[int], \"The start line. Default is 0\"] = None,\n","    end: Annotated[Optional[int], \"The end line. Default is None\"] = None,\n",") -> str:\n","    \"\"\"Read the specified document.\"\"\"\n","    with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n","        lines = file.readlines()\n","    if start is None:\n","        start = 0\n","    return \"\".join(lines[start:end])\n","\n","@tool\n","def write_document(\n","    content: Annotated[str, \"Text content to be written into the document.\"],\n","    file_name: Annotated[str, \"File path to save the document.\"],\n",") -> Annotated[str, \"Path of the saved document file.\"]:\n","    \"\"\"Create and save a text document.\"\"\"\n","    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n","        file.write(content)\n","    return f\"Document saved to {file_name}\"\n","\n","@tool\n","def upload_document(file_path: str) -> str:\n","    \"\"\"Upload and read a document (PDF or text).\"\"\"\n","    file_extension = Path(file_path).suffix.lower()\n","    if file_extension == '.pdf':\n","        with open(file_path, 'rb') as file:\n","            pdf_reader = PyPDF2.PdfReader(file)\n","            text = \"\"\n","            for page in pdf_reader.pages:\n","                text += page.extract_text()\n","    elif file_extension in ['.txt', '.md']:\n","        with open(file_path, 'r') as file:\n","            text = file.read()\n","    else:\n","        raise ValueError(\"Unsupported file type. Please upload a PDF or text file.\")\n","    \n","    # Save the extracted text to a file in the working directory\n","    output_file = WORKING_DIRECTORY / f\"uploaded_document_{uuid.uuid4()}.txt\"\n","    with output_file.open(\"w\") as file:\n","        file.write(text)\n","    \n","    return f\"Document uploaded and saved as {output_file.name}\"\n","\n","@tool\n","def query_vector_db(query: str) -> str:\n","    \"\"\"Query the vector database for relevant information.\"\"\"\n","    results = query_vector_store(query)\n","    return \"\\n\".join([f\"{item['type']}: {item['content']}\" for item in results])"]},{"cell_type":"markdown","metadata":{},"source":["Agent Creation"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def create_agent_node(name: str, tools: List[BaseTool], system_prompt: str):\n","    llm = ChatOpenAI(model=\"gpt-4-turbo\")\n","    prompt = ChatPromptTemplate.from_messages([\n","        (\"system\", system_prompt + \"\\n\\nAfter completing your task, you MUST specify the next step by setting the 'next' value. Use 'END' if you believe the process is complete.\"),\n","        MessagesPlaceholder(variable_name=\"messages\"),\n","        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n","    ])\n","    agent = create_openai_functions_agent(llm, tools, prompt)\n","    executor = AgentExecutor(agent=agent, tools=tools)\n","    return functools.partial(agent_node, agent=prelude | executor, name=name)\n","\n","def agent_node(state, agent, name):\n","    result = agent.invoke(state)\n","    \n","    # Ensure we're always updating 'messages' and 'next'\n","    new_state = {\n","        \"messages\": state[\"messages\"] + [HumanMessage(content=result[\"output\"], name=name)],\n","        \"next\": state.get(\"next\", name),  # Default to the current node if 'next' is not set\n","        \"documents\": state.get(\"documents\", {})  # Preserve any existing documents\n","    }\n","    \n","    # If the agent explicitly set a 'next' value, use that\n","    if \"next\" in result:\n","        new_state[\"next\"] = result[\"next\"]\n","    \n","    return new_state\n","\n","# Define agent nodes\n","company_research_alpha = create_agent_node(\n","    \"CompanyResearchAlpha\",\n","    [tavily_tool, read_document, write_document, upload_document, query_vector_db],\n","    \"You are a company research specialist. Your task is to gather initial information about a target company. \"\n","    \"Ask the user for the company name or to upload a document with company info. \"\n","    \"Create initial documents for company name, description, execs, priorities, and industry priorities.\"\n",")\n","\n","# Company Description Team\n","company_description_research = create_agent_node(\n","    \"CompanyDescriptionResearch\",\n","    [tavily_tool, read_document, write_document, query_vector_db],\n","    \"You are a company description researcher. Your task is to find and compile a comprehensive description of the company.\"\n",")\n","\n","company_description_writing = create_agent_node(\n","    \"CompanyDescriptionWriting\",\n","    [read_document, write_document],\n","    \"You are a professional writer specializing in company descriptions. Your task is to take the research and craft a well-written company description.\"\n",")\n","\n","company_description_fact_checker = create_agent_node(\n","    \"CompanyDescriptionFactChecker\",\n","    [tavily_tool, read_document, write_document, query_vector_db],\n","    \"You are a fact-checker. Your task is to verify the accuracy of the company description.\"\n",")\n","\n","company_description_copy_editor = create_agent_node(\n","    \"CompanyDescriptionCopyEditor\",\n","    [read_document, write_document],\n","    \"You are a copy editor. Your task is to ensure the company description has correct grammar, spelling, and tone.\"\n",")\n","\n","# Company Execs Team\n","company_execs_research = create_agent_node(\n","    \"CompanyExecsResearch\",\n","    [tavily_tool, read_document, write_document, query_vector_db],\n","    \"You are an executive research specialist. Your task is to find the top 5 C-level executives of the company.\"\n",")\n","\n","company_execs_linkedin_check = create_agent_node(\n","    \"CompanyExecsLinkedInCheck\",\n","    [linkedin_check, read_document, write_document],\n","    \"You are a LinkedIn verification specialist. Your task is to verify if the executives are currently working at the company.\"\n",")\n","\n","company_execs_editor = create_agent_node(\n","    \"CompanyExecsEditor\",\n","    [read_document, write_document],\n","    \"You are an editor. Your task is to finalize the list of company executives, excluding any that are not currently with the company.\"\n",")\n","\n","# Company Priorities Team\n","company_priorities_research = create_agent_node(\n","    \"CompanyPrioritiesResearch\",\n","    [tavily_tool, read_document, write_document, query_vector_db],\n","    \"You are a company priorities researcher. Your task is to identify the top 3 priorities of the company.\"\n",")\n","\n","company_priorities_fact_checker = create_agent_node(\n","    \"CompanyPrioritiesFactChecker\",\n","    [tavily_tool, read_document, write_document, query_vector_db],\n","    \"You are a fact-checker. Your task is to verify the accuracy of the identified company priorities.\"\n",")\n","\n","company_priorities_copy_editor = create_agent_node(\n","    \"CompanyPrioritiesCopyEditor\",\n","    [read_document, write_document],\n","    \"You are a copy editor. Your task is to ensure the company priorities are clearly and correctly stated.\"\n",")\n","\n","# Industry Trends Team\n","industry_trends_research = create_agent_node(\n","    \"IndustryTrendsResearch\",\n","    [tavily_tool, read_document, write_document, query_vector_db],\n","    \"You are an industry trends researcher. Your task is to identify the industry of the company and its top trends and challenges.\"\n",")\n","\n","industry_trends_fact_checker = create_agent_node(\n","    \"IndustryTrendsFactChecker\",\n","    [tavily_tool, read_document, write_document, query_vector_db],\n","    \"You are a fact-checker. Your task is to verify the accuracy of the identified industry trends and challenges.\"\n",")\n","\n","industry_trends_copy_editor = create_agent_node(\n","    \"IndustryTrendsCopyEditor\",\n","    [read_document, write_document],\n","    \"You are a copy editor. Your task is to ensure the industry trends and challenges are clearly and correctly stated.\"\n",")\n","\n","# Sales Pitch Team\n","value_proposition_research = create_agent_node(\n","    \"ValuePropositionResearch\",\n","    [tavily_tool, read_document, write_document, upload_document, query_vector_db],\n","    \"You are a value proposition researcher. Your task is to research and outline the value proposition of the user's company.\"\n",")\n","\n","value_proposition_fact_checker = create_agent_node(\n","    \"ValuePropositionFactChecker\",\n","    [tavily_tool, read_document, write_document, query_vector_db],\n","    \"You are a fact-checker. Your task is to verify the accuracy of the value proposition.\"\n",")\n","\n","value_proposition_copy_editor = create_agent_node(\n","    \"ValuePropositionCopyEditor\",\n","    [read_document, write_document],\n","    \"You are a copy editor. Your task is to ensure the value proposition is clearly and correctly stated.\"\n",")\n","\n","sales_pitch_creator = create_agent_node(\n","    \"SalesPitchCreator\",\n","    [read_document, write_document, query_vector_db],\n","    \"You are a sales pitch creator. Your task is to create a compelling sales pitch based on the target company's information and the user's company value proposition.\"\n",")\n","\n","sales_pitch_fact_checker = create_agent_node(\n","    \"SalesPitchFactChecker\",\n","    [read_document, write_document, query_vector_db],\n","    \"You are a fact-checker. Your task is to verify the accuracy of the sales pitch.\"\n",")\n","\n","sales_pitch_copy_editor = create_agent_node(\n","    \"SalesPitchCopyEditor\",\n","    [read_document, write_document],\n","    \"You are a copy editor. Your task is to ensure the sales pitch has correct grammar, spelling, and tone.\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Graph Creation"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["from typing import Dict, List, TypedDict\n","from langgraph.graph import StateGraph, END\n","from langchain_core.messages import HumanMessage\n","\n","# Define the state\n","class State(TypedDict):\n","    messages: List[HumanMessage]\n","    next: str\n","\n","# Create the graph\n","graph = StateGraph(State)\n","\n","# Define a dictionary mapping node names to their corresponding functions\n","node_functions = {\n","    \"CompanyResearchAlpha\": company_research_alpha,\n","    \"CompanyDescriptionResearch\": company_description_research,\n","    \"CompanyDescriptionWriting\": company_description_writing,\n","    \"CompanyDescriptionFactChecker\": company_description_fact_checker,\n","    \"CompanyDescriptionCopyEditor\": company_description_copy_editor,\n","    \"CompanyExecsResearch\": company_execs_research,\n","    \"CompanyExecsLinkedInCheck\": company_execs_linkedin_check,\n","    \"CompanyExecsEditor\": company_execs_editor,\n","    \"CompanyPrioritiesResearch\": company_priorities_research,\n","    \"CompanyPrioritiesFactChecker\": company_priorities_fact_checker,\n","    \"CompanyPrioritiesCopyEditor\": company_priorities_copy_editor,\n","    \"IndustryTrendsResearch\": industry_trends_research,\n","    \"IndustryTrendsFactChecker\": industry_trends_fact_checker,\n","    \"IndustryTrendsCopyEditor\": industry_trends_copy_editor,\n","    \"ValuePropositionResearch\": value_proposition_research,\n","    \"ValuePropositionFactChecker\": value_proposition_fact_checker,\n","    \"ValuePropositionCopyEditor\": value_proposition_copy_editor,\n","    \"SalesPitchCreator\": sales_pitch_creator,\n","    \"SalesPitchFactChecker\": sales_pitch_fact_checker,\n","    \"SalesPitchCopyEditor\": sales_pitch_copy_editor\n","}\n","\n","# Define the node order\n","node_order = [\n","    \"CompanyResearchAlpha\",\n","    \"CompanyResearchBeta\",\n","    \"SalesPitchWriter\",\n","    \"SalesPitchCopyEditor\",\n","    \"SalesPitchDopenessEditor\"\n","]\n","\n","# Add nodes to the graph\n","for node in node_order:\n","    graph.add_node(node, node_functions)\n","\n","# Add edges between nodes\n","for i in range(len(node_order) - 1):\n","    current_node = node_order[i]\n","    next_node = node_order[i + 1]\n","    graph.add_edge(current_node, next_node)\n","\n","# Add conditional edges for each node\n","for i, node in enumerate(node_order):\n","    next_nodes = {next_node: next_node for next_node in node_order[i+1:]}\n","    next_nodes[\"END\"] = END\n","    \n","    graph.add_conditional_edges(\n","        node,\n","        lambda x: x[\"next\"],\n","        next_nodes\n","    )\n","\n","# Set the entry point\n","graph.set_entry_point(\"CompanyResearchAlpha\")\n","\n","# Compile the graph\n","workflow = graph.compile()"]},{"cell_type":"markdown","metadata":{},"source":["Main Chain and Execution"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Generating sales pitch...\n","An error occurred: Must write to at least one of ['messages', 'next']\n","\n","Final Sales Pitch:\n","Final sales pitch file not found. There may have been an error in the process.\n"]}],"source":["# Initialize vector store\n","def initialize_vector_store():\n","    if qdrant_client.collection_exists(\"sales_pitch_data\"):\n","        qdrant_client.delete_collection(\"sales_pitch_data\")\n","    qdrant_client.create_collection(\n","        collection_name=\"sales_pitch_data\",\n","        vectors_config=rest.VectorParams(size=1536, distance=rest.Distance.COSINE),\n","    )\n","\n","# Main execution function\n","def run_sales_pitch_generator(target_company: str, user_company_url: str):\n","    initial_message = f\"Generate a sales pitch for {target_company}. Our company website is {user_company_url}.\"\n","    \n","    state = {\"messages\": [HumanMessage(content=initial_message)], \"next\": \"CompanyResearchAlpha\", \"documents\": {}}\n","    \n","    # Compile the graph\n","    chain = graph.compile()\n","    \n","    while True:\n","        try:\n","            result = chain.invoke(state)\n","            if result[\"next\"] == \"END\":  # Check for END condition\n","                break\n","            state = result\n","            print(f\"Completed step: {result['next']}\")\n","            print(\"---\")\n","        except Exception as e:\n","            print(f\"An error occurred: {e}\")\n","            break\n","\n","    # After the chain completes, read and return the final sales pitch\n","    try:\n","        with open(WORKING_DIRECTORY / \"final_sales_pitch.txt\", \"r\") as file:\n","            return file.read()\n","    except FileNotFoundError:\n","        return \"Final sales pitch file not found. There may have been an error in the process.\"\n","\n","if __name__ == \"__main__\":\n","    initialize_vector_store()\n","    \n","    target_company = input(\"Enter the name of the target company: \")\n","    user_company_url = input(\"Enter your company's URL: \")\n","    \n","    print(\"Generating sales pitch...\")\n","    final_pitch = run_sales_pitch_generator(target_company, user_company_url)\n","    \n","    print(\"\\nFinal Sales Pitch:\")\n","    print(final_pitch)"]}],"metadata":{"kernelspec":{"display_name":"llmops-course","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":2}

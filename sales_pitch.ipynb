{"cells":[{"cell_type":"markdown","metadata":{},"source":["Installing libraries"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install -qU langgraph langchain langchain_openai langchain_community qdrant-client PyPDF2 beautifulsoup4 requests"]},{"cell_type":"markdown","metadata":{},"source":["Setting up imports"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import os\n","import getpass\n","import logging\n","import json\n","import uuid\n","from typing import List, Dict, Any, Union, Annotated, Optional\n","from pathlib import Path\n","from tenacity import retry, stop_after_attempt, wait_exponential\n","\n","from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_core.tools import BaseTool, tool\n","from langgraph.graph import StateGraph, END\n","from langchain_community.tools.tavily_search import TavilySearchResults\n","from langchain_community.vectorstores import Qdrant\n","from qdrant_client import QdrantClient\n","from qdrant_client.http import models as rest\n","from langchain.agents import Tool, create_openai_functions_agent, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n","from langchain.prompts import StringPromptTemplate\n","from langchain.chains import LLMChain\n","from langchain.schema import AgentAction, AgentFinish, HumanMessage\n","from langchain.chat_models import ChatOpenAI\n","import re\n","from typing import List, Union, Tuple\n","\n","# Set up API keys\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n","os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API Key:\")"]},{"cell_type":"markdown","metadata":{},"source":["Create a working directory"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Create a working directory\n","WORKING_DIRECTORY = Path(\"./content/data\")\n","WORKING_DIRECTORY.mkdir(parents=True, exist_ok=True)"]},{"cell_type":"markdown","metadata":{},"source":["Initialize Qdrant Client and OpenAI Embeddings"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"ename":"ValidationError","evalue":"1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize Qdrant client and OpenAI embeddings\u001b[39;00m\n\u001b[1;32m      2\u001b[0m qdrant_client \u001b[38;5;241m=\u001b[39m QdrantClient(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:memory:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/anaconda3/envs/llmops-course/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n","\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"]}],"source":["# Initialize Qdrant client and OpenAI embeddings\n","qdrant_client = QdrantClient(\":memory:\")\n","embeddings = OpenAIEmbeddings()"]},{"cell_type":"markdown","metadata":{},"source":["State and Helper functions"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[],"source":["class State(Dict[str, Any]):\n","    messages: Annotated[List[BaseMessage], operator.add]\n","    next: str\n","    documents: Dict[str, str]\n","\n","def get_last_message(state: State) -> str:\n","    return state[\"messages\"][-1].content\n","\n","def join_graph(response: dict):\n","    return {\"messages\": [response[\"messages\"][-1]]}\n","\n","def prelude(state):\n","    written_files = []\n","    try:\n","        written_files = [f.relative_to(WORKING_DIRECTORY) for f in WORKING_DIRECTORY.glob(\"*\")]\n","    except:\n","        pass\n","    if not written_files:\n","        return {**state, \"current_files\": \"No files written.\"}\n","    return {\n","        **state,\n","        \"current_files\": \"\\nBelow are files your team has written to the directory:\\n\"\n","        + \"\\n\".join([f\" - {f}\" for f in written_files]),\n","    }"]},{"cell_type":"markdown","metadata":{},"source":["Setting up Vector Store"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[],"source":["def initialize_vector_store():\n","    qdrant_client.recreate_collection(\n","        collection_name=\"sales_pitch_data\",\n","        vectors_config=rest.VectorParams(size=1536, distance=rest.Distance.COSINE),\n","    )\n","\n","def add_to_vector_store(text: str, metadata: Dict[str, Any]):\n","    vector = embeddings.embed_query(text)\n","    qdrant_client.upsert(\n","        collection_name=\"sales_pitch_data\",\n","        points=[rest.PointStruct(id=uuid.uuid4().hex, vector=vector, payload=metadata)],\n","    )\n","\n","def query_vector_store(query: str, limit: int = 5) -> List[Dict[str, Any]]:\n","    query_vector = embeddings.embed_query(query)\n","    search_result = qdrant_client.search(\n","        collection_name=\"sales_pitch_data\",\n","        query_vector=query_vector,\n","        limit=limit,\n","    )\n","    return [hit.payload for hit in search_result]"]},{"cell_type":"markdown","metadata":{},"source":["Tools"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[],"source":["tavily_tool = TavilySearchResults(max_results=5)\n","\n","@tool\n","def linkedin_check(name: str, company: str) -> bool:\n","    \"\"\"Check if a person works at a specific company on LinkedIn.\"\"\"\n","    # Note: This is a mock implementation. In a real scenario, you'd use LinkedIn's API or web scraping.\n","    url = f\"https://www.linkedin.com/search/results/people/?keywords={name} {company}\"\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.content, 'html.parser')\n","    return company.lower() in soup.text.lower()\n","\n","@tool\n","def read_document(\n","    file_name: Annotated[str, \"File path to read the document.\"],\n","    start: Annotated[Optional[int], \"The start line. Default is 0\"] = None,\n","    end: Annotated[Optional[int], \"The end line. Default is None\"] = None,\n",") -> str:\n","    \"\"\"Read the specified document.\"\"\"\n","    with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n","        lines = file.readlines()\n","    if start is None:\n","        start = 0\n","    return \"\".join(lines[start:end])\n","\n","@tool\n","def write_document(\n","    content: Annotated[str, \"Text content to be written into the document.\"],\n","    state: Annotated[dict, \"The current state of the conversation.\"],\n",") -> Annotated[str, \"Path of the saved document file or error message.\"]:\n","    \"\"\"Create and save a text document.\"\"\"\n","    file_name = state.get(\"output_file\", \"output.txt\")\n","    file_path = WORKING_DIRECTORY / file_name\n","    logging.info(f\"Attempting to write document: {file_path}\")\n","    try:\n","        logging.debug(f\"Content to write: {content[:100]}...\")  # Log first 100 chars\n","        \n","        # Ensure the directory exists\n","        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n","        \n","        with file_path.open(\"w\") as file:\n","            file.write(content)\n","        logging.info(f\"Successfully wrote content to {file_path}\")\n","        return f\"Document saved to {file_path}\"\n","    except IOError as e:\n","        error_msg = f\"IOError writing to {file_path}: {e}\"\n","        logging.error(error_msg)\n","        return error_msg\n","    except Exception as e:\n","        error_msg = f\"Unexpected error writing to {file_path}: {e}\"\n","        logging.error(error_msg)\n","        return error_msg\n","\n","@tool\n","def upload_document(file_path: str) -> str:\n","    \"\"\"Upload and read a document (PDF or text).\"\"\"\n","    file_extension = Path(file_path).suffix.lower()\n","    if file_extension == '.pdf':\n","        with open(file_path, 'rb') as file:\n","            pdf_reader = PyPDF2.PdfReader(file)\n","            text = \"\"\n","            for page in pdf_reader.pages:\n","                text += page.extract_text()\n","    elif file_extension in ['.txt', '.md']:\n","        with open(file_path, 'r') as file:\n","            text = file.read()\n","    else:\n","        raise ValueError(\"Unsupported file type. Please upload a PDF or text file.\")\n","    \n","    # Save the extracted text to a file in the working directory\n","    output_file = WORKING_DIRECTORY / f\"uploaded_document_{uuid.uuid4()}.txt\"\n","    with output_file.open(\"w\") as file:\n","        file.write(text)\n","    \n","    return f\"Document uploaded and saved as {output_file.name}\"\n","\n","@tool\n","def query_vector_db(query: str) -> str:\n","    \"\"\"Query the vector database for relevant information.\"\"\"\n","    results = query_vector_store(query)\n","    return \"\\n\".join([f\"{item['type']}: {item['content']}\" for item in results])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_agent(name, tools, prompt):\n","    # Define which tools the agent can use\n","    agent_tools = tools\n","\n","    # Set up the base template\n","    template = prompt + \"\"\"\n","\n","    Human: {human_input}\n","\n","    Assistant: Certainly! I'll use the available tools to gather information and complete the task. Let's get started.\n","\n","    {agent_scratchpad}\"\"\"\n","\n","    # Set up a prompt template\n","    class CustomPromptTemplate(StringPromptTemplate):\n","        template: str\n","        tools: List[Tool]\n","\n","        def format(self, **kwargs) -> str:\n","            intermediate_steps = kwargs.pop(\"intermediate_steps\")\n","            thoughts = \"\"\n","            for action, observation in intermediate_steps:\n","                thoughts += action.log\n","                thoughts += f\"\\nObservation: {observation}\\nThought: \"\n","            kwargs[\"agent_scratchpad\"] = thoughts\n","            kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n","            return self.template.format(**kwargs)\n","\n","    prompt_template = CustomPromptTemplate(\n","        template=template,\n","        tools=agent_tools,\n","        input_variables=[\"human_input\", \"intermediate_steps\"]\n","    )\n","\n","    # Define how the agent should output its actions\n","    class CustomOutputParser(AgentOutputParser):\n","        def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n","            if \"Final Answer:\" in llm_output:\n","                return AgentFinish(\n","                    return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n","                    log=llm_output,\n","                )\n","            regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n","            match = re.search(regex, llm_output, re.DOTALL)\n","            if not match:\n","                raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n","            action = match.group(1).strip()\n","            action_input = match.group(2)\n","            return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n","\n","    output_parser = CustomOutputParser()\n","\n","    # Define the LLM to use\n","    llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\n","\n","    # Create the LLM chain\n","    llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n","\n","    # Define the agent\n","    agent = LLMSingleActionAgent(\n","        llm_chain=llm_chain,\n","        output_parser=output_parser,\n","        stop=[\"\\nObservation:\"],\n","        allowed_tools=[tool.name for tool in agent_tools]\n","    )\n","\n","    # Create the agent executor\n","    agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=agent_tools, verbose=True)\n","\n","    return agent_executor"]},{"cell_type":"markdown","metadata":{},"source":["Agent Creation"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[],"source":["def create_agent_node(name, tools, prompt):\n","    def wrapper(state):\n","        try:\n","            agent = create_agent(name, tools, prompt)\n","            agent_output = agent.run(state[\"messages\"][-1].content)\n","            \n","            next_step_index = node_order.index(name) + 1\n","            next_step = node_order[next_step_index] if next_step_index < len(node_order) else \"END\"\n","            \n","            # Write the output to a file\n","            output_file = f\"{name}.txt\"\n","            with open(output_file, 'w') as f:\n","                f.write(agent_output)\n","            \n","            return {\n","                \"messages\": state[\"messages\"] + [HumanMessage(content=agent_output)],\n","                \"next\": next_step,\n","                \"documents\": {**state.get(\"documents\", {}), name: output_file},\n","                \"last_output_file\": output_file\n","            }\n","        except Exception as e:\n","            logging.error(f\"Error in {name}: {str(e)}\")\n","            return {\n","                \"messages\": state[\"messages\"] + [HumanMessage(content=f\"Error in {name}: {str(e)}\")],\n","                \"next\": \"END\",\n","                \"documents\": {**state.get(\"documents\", {}), name: f\"Error: {str(e)}\"},\n","                \"last_output_file\": None\n","            }\n","    \n","    return wrapper"]},{"cell_type":"markdown","metadata":{},"source":["ComplanyResearchAlpha Function Definition"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[],"source":["# Define the tools that CompanyResearchAlpha will use\n","company_research_tools = [\n","    Tool(\n","        name=\"Tavily Search\",\n","        func=tavily_tool.run,\n","        description=\"Useful for searching the internet for up-to-date information on companies and industries.\"\n","    ),\n","    Tool(\n","        name=\"Read Document\",\n","        func=read_document,\n","        description=\"Useful for reading the content of a document.\"\n","    ),\n","    Tool(\n","        name=\"Write Document\",\n","        func=write_document,\n","        description=\"Useful for writing content to a document.\"\n","    ),\n","    Tool(\n","        name=\"Upload Document\",\n","        func=upload_document,\n","        description=\"Useful for uploading a document to the vector database.\"\n","    ),\n","    Tool(\n","        name=\"Query Vector DB\",\n","        func=query_vector_db,\n","        description=\"Useful for querying the vector database for relevant information.\"\n","    )\n","]\n","\n","# Create the CompanyResearchAlpha node\n","company_research_alpha = create_agent_node(\n","    \"CompanyResearchAlpha\",\n","    company_research_tools,\n","    \"\"\"You are a company research specialist. Your task is to gather initial information about the target company.\n","    Use the available tools to research the company and compile a brief summary including:\n","    1. Company name and basic description\n","    2. Industry\n","    3. Key products or services\n","    4. Recent news or developments\n","\n","    After gathering this information, use the write_document tool to save your findings in a file named 'CompanyResearchAlpha.txt'.\n","    Your response should include confirmation that the file was saved.\"\"\"\n",")\n","\n","# Add this to your node_functions dictionary\n","node_functions[\"CompanyResearchAlpha\"] = company_research_alpha"]},{"cell_type":"markdown","metadata":{},"source":["Defining Agent Nodes"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[],"source":["# Define agent nodes\n","company_research_alpha = create_agent_node(\n","    \"CompanyResearchAlpha\",\n","    [tavily_tool, read_document, write_document, upload_document, query_vector_db],\n","    \"You are a company research specialist. Your task is to gather initial information about a target company. \"\n","    \"Ask the user for the company name or to upload a document with company info. \"\n","    \"Create initial documents for company name, description, execs, priorities, and industry priorities.\"\n",")\n","\n","# Company Description Team\n","company_description_research = create_agent_node(\n","    \"CompanyDescriptionResearch\",\n","    [tavily_tool, read_document, write_document, query_vector_db],\n","    \"You are a company description researcher. Your task is to find and compile a comprehensive description of the company.\"\n",")\n","\n","company_description_writing = create_agent_node(\n","    \"CompanyDescriptionWriting\",\n","    [read_document, write_document],\n","    \"You are a professional writer specializing in company descriptions. Your task is to take the research and craft a well-written company description.\"\n",")\n","\n","company_description_fact_checker = create_agent_node(\n","    \"CompanyDescriptionFactChecker\",\n","    [tavily_tool, read_document, write_document, query_vector_db],\n","    \"You are a fact-checker. Your task is to verify the accuracy of the company description.\"\n",")\n","\n","company_description_copy_editor = create_agent_node(\n","    \"CompanyDescriptionCopyEditor\",\n","    [read_document, write_document],\n","    \"\"\"You are a copy editor. Your task is to ensure the company description has correct grammar, spelling, and tone.\n","    Read the existing company description, make necessary edits, and save the improved version.\n","    If you encounter any issues with the input, please provide a clear explanation and save it in the output file.\"\"\"\n",")\n","\n","# Company Execs Team\n","company_execs_research = create_agent_node(\n","    \"CompanyExecsResearch\",\n","    [tavily_tool, read_document, write_document, query_vector_db],\n","    \"You are an executive research specialist. Your task is to find the top 5 C-level executives of the company.\"\n",")\n","\n","company_execs_linkedin_check = create_agent_node(\n","    \"CompanyExecsLinkedInCheck\",\n","    [linkedin_check, read_document, write_document],\n","    \"You are a LinkedIn verification specialist. Your task is to verify if the executives are currently working at the company.\"\n",")\n","\n","company_execs_editor = create_agent_node(\n","    \"CompanyExecsEditor\",\n","    [read_document, write_document],\n","    \"You are an editor. Your task is to finalize the list of company executives, excluding any that are not currently with the company.\"\n",")\n","\n","# Company Priorities Team\n","company_priorities_research = create_agent_node(\n","    \"CompanyPrioritiesResearch\",\n","    [tavily_tool, read_document, write_document, query_vector_db],\n","    \"You are a company priorities researcher. Your task is to identify the top 3 priorities of the company.\"\n",")\n","\n","company_priorities_fact_checker = create_agent_node(\n","    \"CompanyPrioritiesFactChecker\",\n","    [tavily_tool, read_document, write_document, query_vector_db],\n","    \"You are a fact-checker. Your task is to verify the accuracy of the identified company priorities.\"\n",")\n","\n","company_priorities_copy_editor = create_agent_node(\n","    \"CompanyPrioritiesCopyEditor\",\n","    [read_document, write_document],\n","    \"You are a copy editor. Your task is to ensure the company priorities are clearly and correctly stated.\"\n",")\n","\n","# Industry Trends Team\n","industry_trends_research = create_agent_node(\n","    \"IndustryTrendsResearch\",\n","    [tavily_tool, read_document, write_document, query_vector_db],\n","    \"You are an industry trends researcher. Your task is to identify the industry of the company and its top trends and challenges.\"\n",")\n","\n","industry_trends_fact_checker = create_agent_node(\n","    \"IndustryTrendsFactChecker\",\n","    [tavily_tool, read_document, write_document, query_vector_db],\n","    \"You are a fact-checker. Your task is to verify the accuracy of the identified industry trends and challenges.\"\n",")\n","\n","industry_trends_copy_editor = create_agent_node(\n","    \"IndustryTrendsCopyEditor\",\n","    [read_document, write_document],\n","    \"You are a copy editor. Your task is to ensure the industry trends and challenges are clearly and correctly stated.\"\n",")\n","\n","# Sales Pitch Team\n","value_proposition_research = create_agent_node(\n","    \"ValuePropositionResearch\",\n","    [tavily_tool, read_document, write_document, upload_document, query_vector_db],\n","    \"You are a value proposition researcher. Your task is to research and outline the value proposition of the user's company.\"\n",")\n","\n","value_proposition_fact_checker = create_agent_node(\n","    \"ValuePropositionFactChecker\",\n","    [tavily_tool, read_document, write_document, query_vector_db],\n","    \"You are a fact-checker. Your task is to verify the accuracy of the value proposition.\"\n",")\n","\n","value_proposition_copy_editor = create_agent_node(\n","    \"ValuePropositionCopyEditor\",\n","    [read_document, write_document],\n","    \"You are a copy editor. Your task is to ensure the value proposition is clearly and correctly stated.\"\n",")\n","\n","sales_pitch_creator = create_agent_node(\n","    \"SalesPitchCreator\",\n","    [read_document, write_document, query_vector_db],\n","    \"\"\"You are a sales pitch creator. Your task is to create a compelling sales pitch based on the target company's information and the user's company value proposition.\n","    After creating the pitch, you MUST use the write_document tool to save it as 'final_sales_pitch.txt'.\n","    Your response should include the content of the sales pitch and confirmation that it was saved.\"\"\"\n",")\n","\n","sales_pitch_fact_checker = create_agent_node(\n","    \"SalesPitchFactChecker\",\n","    [read_document, write_document, query_vector_db],\n","    \"You are a fact-checker. Your task is to verify the accuracy of the sales pitch.\"\n",")\n","\n","sales_pitch_copy_editor = create_agent_node(\n","    \"SalesPitchCopyEditor\",\n","    [read_document, write_document],\n","    \"You are a copy editor. Your task is to ensure the sales pitch has correct grammar, spelling, and tone.\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Graph Creation"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[],"source":["from typing import Dict, List, TypedDict\n","from langgraph.graph import StateGraph, END\n","from langchain_core.messages import HumanMessage\n","\n","# Define the state\n","class State(TypedDict):\n","    messages: List[HumanMessage]\n","    next: str\n","\n","# Create the graph\n","graph = StateGraph(State)\n","\n","# Define a dictionary mapping node names to their corresponding functions\n","node_functions = {\n","    \"CompanyResearchAlpha\": company_research_alpha,\n","    \"CompanyDescriptionResearch\": company_description_research,\n","    \"CompanyDescriptionWriting\": company_description_writing,\n","    \"CompanyDescriptionFactChecker\": company_description_fact_checker,\n","    \"CompanyDescriptionCopyEditor\": company_description_copy_editor,\n","    \"CompanyExecsResearch\": company_execs_research,\n","    \"CompanyExecsLinkedInCheck\": company_execs_linkedin_check,\n","    \"CompanyExecsEditor\": company_execs_editor,\n","    \"CompanyPrioritiesResearch\": company_priorities_research,\n","    \"CompanyPrioritiesFactChecker\": company_priorities_fact_checker,\n","    \"CompanyPrioritiesCopyEditor\": company_priorities_copy_editor,\n","    \"IndustryTrendsResearch\": industry_trends_research,\n","    \"IndustryTrendsFactChecker\": industry_trends_fact_checker,\n","    \"IndustryTrendsCopyEditor\": industry_trends_copy_editor,\n","    \"ValuePropositionResearch\": value_proposition_research,\n","    \"ValuePropositionFactChecker\": value_proposition_fact_checker,\n","    \"ValuePropositionCopyEditor\": value_proposition_copy_editor,\n","    \"SalesPitchCreator\": sales_pitch_creator,\n","    \"SalesPitchFactChecker\": sales_pitch_fact_checker,\n","    \"SalesPitchCopyEditor\": sales_pitch_copy_editor\n","}\n","\n","# Define the node order\n","node_order = [\n","    \"CompanyResearchAlpha\",\n","    \"CompanyDescriptionResearch\",\n","    \"CompanyDescriptionWriting\",\n","    \"CompanyDescriptionFactChecker\",\n","    \"CompanyDescriptionCopyEditor\",\n","    \"CompanyExecsResearch\",\n","    \"CompanyExecsLinkedInCheck\",\n","    \"CompanyExecsEditor\",\n","    \"CompanyPrioritiesResearch\",\n","    \"CompanyPrioritiesFactChecker\",\n","    \"CompanyPrioritiesCopyEditor\",\n","    \"IndustryTrendsResearch\",\n","    \"IndustryTrendsFactChecker\",\n","    \"IndustryTrendsCopyEditor\",\n","    \"ValuePropositionResearch\",\n","    \"ValuePropositionFactChecker\",\n","    \"ValuePropositionCopyEditor\",\n","    \"SalesPitchCreator\",\n","    \"SalesPitchFactChecker\",\n","    \"SalesPitchCopyEditor\"\n","]\n","\n","# Add nodes to the graph\n","for node in node_order:\n","    graph.add_node(node, node_functions[node])\n","\n","# Add edges between nodes\n","for i in range(len(node_order) - 1):\n","    current_node = node_order[i]\n","    next_node = node_order[i + 1]\n","    graph.add_edge(current_node, next_node)\n","\n","# Add conditional edges for each node\n","for i, node in enumerate(node_order):\n","    next_nodes = {next_node: next_node for next_node in node_order[i+1:]}\n","    next_nodes[\"END\"] = END\n","    \n","    graph.add_conditional_edges(\n","        node,\n","        lambda x: x[\"next\"],\n","        next_nodes\n","    )\n","\n","# Set the entry point\n","graph.set_entry_point(\"CompanyResearchAlpha\")\n","\n","# Compile the graph\n","workflow = graph.compile()"]},{"cell_type":"markdown","metadata":{},"source":["Sales Pitch Generator Function"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[],"source":["def run_sales_pitch_generator(target_company: str, user_company_url: str, debug: bool = False):\n","    if debug:\n","        logging.getLogger().setLevel(logging.DEBUG)\n","    \n","    initial_message = f\"Generate a sales pitch for {target_company}. Our company website is {user_company_url}.\"\n","    \n","    state = {\"messages\": [HumanMessage(content=initial_message)], \"next\": \"CompanyResearchAlpha\", \"documents\": {}}\n","    \n","    for step, node_name in enumerate(node_order):\n","        logging.info(f\"Executing step {step + 1}/{len(node_order)}: {node_name}\")\n","        \n","        if node_name not in node_functions:\n","            logging.error(f\"Node '{node_name}' not found in node_functions dictionary\")\n","            continue\n","        \n","        node_function = node_functions[node_name]\n","        try:\n","            new_state = node_function(state)\n","            if new_state is None or not isinstance(new_state, dict) or 'next' not in new_state:\n","                raise ValueError(f\"Invalid state returned by {node_name}\")\n","            logging.debug(f\"State after {node_name}:\")\n","            logging.debug(json.dumps(new_state, default=str, indent=2))\n","            state = new_state\n","        except Exception as e:\n","            logging.error(f\"Error in {node_name}: {str(e)}\")\n","            state = {\n","                \"messages\": state[\"messages\"] + [AIMessage(content=f\"Error in {node_name}: {str(e)}\")],\n","                \"next\": \"END\",\n","                \"documents\": {**state.get(\"documents\", {}), node_name: f\"Error: {str(e)}\"},\n","                \"last_output_file\": None\n","            }\n","        \n","        if state['next'] == \"END\":\n","            logging.warning(f\"Process ended early at step {step + 1} ({node_name})\")\n","            break\n","\n","    # After the chain completes, compile all the information into a final sales pitch\n","    final_pitch = compile_final_sales_pitch(state['documents'], target_company)\n","    \n","    # Save the final pitch\n","    final_pitch_path = WORKING_DIRECTORY / \"final_sales_pitch.txt\"\n","    try:\n","        with final_pitch_path.open(\"w\") as file:\n","            file.write(final_pitch)\n","        logging.info(\"Final sales pitch saved successfully.\")\n","    except IOError as e:\n","        logging.error(f\"Error saving final sales pitch: {e}\")\n","    \n","    return final_pitch"]},{"cell_type":"markdown","metadata":{},"source":["Main Chain and Execution"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-09-03 16:12:17,704 - INFO - Initializing vector store...\n","/var/folders/jr/_qkyxp313390z32jmym7j9sm0000gp/T/ipykernel_64437/553466154.py:2: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n","  qdrant_client.recreate_collection(\n","2024-09-03 16:12:23,600 - INFO - Generating sales pitch for Amazon...\n","2024-09-03 16:12:23,601 - INFO - Executing step 1/20: CompanyResearchAlpha\n","2024-09-03 16:12:23,601 - ERROR - Error in CompanyResearchAlpha: name 'create_agent' is not defined\n","2024-09-03 16:12:23,601 - DEBUG - State after CompanyResearchAlpha:\n","2024-09-03 16:12:23,602 - DEBUG - {\n","  \"messages\": [\n","    \"content='Generate a sales pitch for Amazon. Our company website is www.snowflake.com.'\",\n","    \"content=\\\"Error in CompanyResearchAlpha: name 'create_agent' is not defined\\\"\"\n","  ],\n","  \"next\": \"END\",\n","  \"documents\": {\n","    \"CompanyResearchAlpha\": \"Error: name 'create_agent' is not defined\"\n","  },\n","  \"last_output_file\": null\n","}\n","2024-09-03 16:12:23,602 - WARNING - Process ended early at step 1 (CompanyResearchAlpha)\n","2024-09-03 16:12:23,603 - INFO - Final sales pitch saved successfully.\n","2024-09-03 16:12:23,603 - INFO - \n","Final Sales Pitch:\n","2024-09-03 16:12:23,604 - WARNING - File CompanyResearchAlpha.txt was not created during the process\n","2024-09-03 16:12:23,604 - WARNING - File CompanyDescriptionResearch.txt was not created during the process\n","2024-09-03 16:12:23,604 - WARNING - File CompanyDescriptionWriting.txt was not created during the process\n","2024-09-03 16:12:23,604 - WARNING - File CompanyDescriptionFactChecker.txt was not created during the process\n","2024-09-03 16:12:23,605 - WARNING - File CompanyDescriptionCopyEditor.txt was not created during the process\n","2024-09-03 16:12:23,605 - WARNING - File CompanyExecsResearch.txt was not created during the process\n","2024-09-03 16:12:23,605 - WARNING - File CompanyExecsLinkedInCheck.txt was not created during the process\n","2024-09-03 16:12:23,605 - WARNING - File CompanyExecsEditor.txt was not created during the process\n","2024-09-03 16:12:23,606 - WARNING - File CompanyPrioritiesResearch.txt was not created during the process\n","2024-09-03 16:12:23,606 - WARNING - File CompanyPrioritiesFactChecker.txt was not created during the process\n","2024-09-03 16:12:23,606 - WARNING - File CompanyPrioritiesCopyEditor.txt was not created during the process\n","2024-09-03 16:12:23,607 - WARNING - File IndustryTrendsResearch.txt was not created during the process\n","2024-09-03 16:12:23,607 - WARNING - File IndustryTrendsFactChecker.txt was not created during the process\n","2024-09-03 16:12:23,608 - WARNING - File IndustryTrendsCopyEditor.txt was not created during the process\n","2024-09-03 16:12:23,608 - WARNING - File ValuePropositionResearch.txt was not created during the process\n","2024-09-03 16:12:23,609 - WARNING - File ValuePropositionFactChecker.txt was not created during the process\n","2024-09-03 16:12:23,609 - WARNING - File ValuePropositionCopyEditor.txt was not created during the process\n","2024-09-03 16:12:23,610 - WARNING - File SalesPitchCreator.txt was not created during the process\n","2024-09-03 16:12:23,610 - WARNING - File SalesPitchFactChecker.txt was not created during the process\n","2024-09-03 16:12:23,610 - WARNING - File SalesPitchCopyEditor.txt was not created during the process\n","2024-09-03 16:12:23,611 - INFO - Cleaning up temporary files...\n","2024-09-03 16:12:23,611 - WARNING - File CompanyResearchAlpha.txt not found during cleanup\n","2024-09-03 16:12:23,612 - WARNING - File CompanyDescriptionResearch.txt not found during cleanup\n","2024-09-03 16:12:23,612 - WARNING - File CompanyDescriptionWriting.txt not found during cleanup\n","2024-09-03 16:12:23,613 - WARNING - File CompanyDescriptionFactChecker.txt not found during cleanup\n","2024-09-03 16:12:23,613 - WARNING - File CompanyDescriptionCopyEditor.txt not found during cleanup\n","2024-09-03 16:12:23,613 - WARNING - File CompanyExecsResearch.txt not found during cleanup\n","2024-09-03 16:12:23,614 - WARNING - File CompanyExecsLinkedInCheck.txt not found during cleanup\n","2024-09-03 16:12:23,614 - WARNING - File CompanyExecsEditor.txt not found during cleanup\n","2024-09-03 16:12:23,615 - WARNING - File CompanyPrioritiesResearch.txt not found during cleanup\n","2024-09-03 16:12:23,615 - WARNING - File CompanyPrioritiesFactChecker.txt not found during cleanup\n","2024-09-03 16:12:23,615 - WARNING - File CompanyPrioritiesCopyEditor.txt not found during cleanup\n","2024-09-03 16:12:23,615 - WARNING - File IndustryTrendsResearch.txt not found during cleanup\n","2024-09-03 16:12:23,616 - WARNING - File IndustryTrendsFactChecker.txt not found during cleanup\n","2024-09-03 16:12:23,616 - WARNING - File IndustryTrendsCopyEditor.txt not found during cleanup\n","2024-09-03 16:12:23,617 - WARNING - File ValuePropositionResearch.txt not found during cleanup\n","2024-09-03 16:12:23,617 - WARNING - File ValuePropositionFactChecker.txt not found during cleanup\n","2024-09-03 16:12:23,617 - WARNING - File ValuePropositionCopyEditor.txt not found during cleanup\n","2024-09-03 16:12:23,617 - WARNING - File SalesPitchCreator.txt not found during cleanup\n","2024-09-03 16:12:23,618 - WARNING - File SalesPitchFactChecker.txt not found during cleanup\n","2024-09-03 16:12:23,618 - WARNING - File SalesPitchCopyEditor.txt not found during cleanup\n","2024-09-03 16:12:23,618 - INFO - Process completed.\n"]},{"name":"stdout","output_type":"stream","text":["Final Sales Pitch for Amazon\n","\n","Company Overview:\n","Error: name 'create_agent' is not defined\n","\n","Key Executives: Information not available.\n","\n","Company Priorities: Information not available.\n","\n","Value Proposition: Information not available.\n","\n","Final Pitch: Information not available.\n","\n","\n"]}],"source":["if __name__ == \"__main__\":\n","    try:\n","        logging.info(\"Initializing vector store...\")\n","        initialize_vector_store()\n","        \n","        target_company = input(\"Enter the name of the company you want to prospect: \").strip()\n","        user_company_url = input(\"Enter your company's URL so we can understand your value proposition: \").strip()\n","        \n","        logging.info(f\"Generating sales pitch for {target_company}...\")\n","        final_pitch = run_sales_pitch_generator(target_company, user_company_url)\n","        \n","        logging.info(\"\\nFinal Sales Pitch:\")\n","        print(final_pitch)\n","        \n","        # Print contents of all generated files\n","        for node_name in node_order:\n","            file_name = f\"{node_name}.txt\"\n","            file_path = WORKING_DIRECTORY / file_name\n","            if file_path.exists():\n","                try:\n","                    with file_path.open(\"r\") as file:\n","                        content = file.read().strip()\n","                        logging.info(f\"\\nContents of {file_name}:\\n{content[:500]}...\\n---\")\n","                except IOError as e:\n","                    logging.error(f\"Error reading {file_name}: {e}\")\n","            else:\n","                logging.warning(f\"File {file_name} was not created during the process\")\n","        \n","    except Exception as e:\n","        logging.error(f\"An unexpected error occurred: {e}\")\n","        logging.exception(\"Detailed traceback:\")\n","    \n","    finally:\n","        logging.info(\"Cleaning up temporary files...\")\n","        for node_name in node_order:\n","            file_name = f\"{node_name}.txt\"\n","            file_path = WORKING_DIRECTORY / file_name\n","            try:\n","                file_path.unlink()\n","                logging.info(f\"Deleted {file_name}\")\n","            except FileNotFoundError:\n","                logging.warning(f\"File {file_name} not found during cleanup\")\n","        \n","        logging.info(\"Process completed.\")"]}],"metadata":{"kernelspec":{"display_name":"llmops-course","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":2}
